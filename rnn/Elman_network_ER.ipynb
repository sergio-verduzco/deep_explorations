{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f899da4c-58d9-49c6-b00a-36ac39d4386a",
   "metadata": {},
   "source": [
    "# Elman_network_ER.ipynb\n",
    "\n",
    "Using an Elman network to test Error Regression mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c50c7-fc80-44fe-97ec-1f32326cef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# import lovely_tensors as lt\n",
    "# lt.monkey_patch()\n",
    "# Expects to be in '/home/z/OIST Dropbox/Sergio Verduzco/code/python/pytorch/deep_explorations/rnn'\n",
    "%cd ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c73891-287c-46b1-8f40-8f5ef8b3e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff80c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_length(seq, des_len):\n",
    "    \"\"\" Change the sequence so it has the desired length. \"\"\"\n",
    "    seq_len = seq.shape[0]\n",
    "    if des_len > seq_len:  # we will repeat a set of random points\n",
    "        new_seq = torch.zeros((des_len,) + seq.shape[1:], dtype=seq.dtype)\n",
    "        new_idxs = torch.randint(0, seq_len-1, (des_len - seq_len,))\n",
    "        idx = 0\n",
    "        for i in range(seq_len):\n",
    "            new_seq[idx] = seq[i]\n",
    "            if i in new_idxs:\n",
    "                new_seq[idx] = seq[i+1]\n",
    "                idx += 1\n",
    "            idx += 1\n",
    "    elif des_len < seq_len:  # we will remove a set of random points\n",
    "        new_seq = torch.zeros((des_len,) + seq.shape[1:], dtype=seq.dtype)\n",
    "        remove_idxs = torch.randperm(seq_len)[:seq_len-des_len]\n",
    "        idx = 0\n",
    "        for i in range(seq_len):\n",
    "            if i in remove_idxs:\n",
    "                continue\n",
    "            else:\n",
    "                new_seq[idx] = seq[i]\n",
    "                idx += 1\n",
    "    else:\n",
    "        new_seq = seq\n",
    "    return new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69055a0f-895d-4bd5-8bc4-15eaa6ad0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "seq_len = 940\n",
    "fname = 'switch_circle5_triangle5_1.pkl'\n",
    "with open(fname, 'rb') as f:\n",
    "    c1 = torch.tensor(pickle.load(f))\n",
    "    c1 = set_length(c1, seq_len)\n",
    "    print(f\"c1 shape: {c1.shape}\")\n",
    "\n",
    "fname = 'infty10_1.pkl'\n",
    "with open(fname, 'rb') as f:\n",
    "    c2 = torch.tensor(pickle.load(f))\n",
    "    c2 = set_length(c2, seq_len)\n",
    "    print(f\"c2 shape: {c2.shape}\")\n",
    "\n",
    "fname = 'eye.pkl'\n",
    "with open(fname, 'rb') as f:\n",
    "    c3 = torch.tensor(pickle.load(f))\n",
    "    c3 = set_length(c3, seq_len)\n",
    "    print(f\"c3 shape: {c3.shape}\")\n",
    "\n",
    "fname = 'switch_circle2_circle2.pkl'\n",
    "with open(fname, 'rb') as f:\n",
    "    c4 = torch.tensor(pickle.load(f))\n",
    "    c4 = set_length(c4, seq_len)\n",
    "    print(f\"c4 shape: {c4.shape}\")\n",
    "\n",
    "coordinates = torch.stack([c1, c2, c3, c4], dim=0)\n",
    "# coordinates = coordinates[:2].to(device).type(torch.float)\n",
    "# put each pattern in a different part of the plane\n",
    "# coordinates[0, :, 0] -= 2.0  # first pattern moves left\n",
    "# coordinates[1, :, 0] += 2.0  # second pattern moves right\n",
    "# coordinates[2, :, 1] += 2.0  # third pattern moves up\n",
    "# coordinates[3, :, 1] -= 2.0  # fourth pattern moves down\n",
    "\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd62d05-7327-469d-bfb0-f1a4ddb6dce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the loaded array\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "for col in range(4):\n",
    "    axs[col].axis('equal')\n",
    "    axs[col].plot(coordinates[col,:, 0].cpu().numpy(), coordinates[col,:, 1].cpu().numpy())\n",
    "\n",
    "coordinates = torch.tensor(coordinates, dtype=torch.float32, device=device)\n",
    "print(f\"Coordinates shape: {coordinates.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2deea-4353-42ac-81e1-9901c68c7344",
   "metadata": {},
   "source": [
    "## The Elman RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60bc16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, the optimizer, and the loss function\n",
    "input_size = 2  # this shouldn't change\n",
    "hidden_size = 256\n",
    "nonlinearity = 'tanh'  # tanh or relu\n",
    "lr = 3e-4  # learning rate\n",
    "all_generated = []\n",
    "all_generated_er = []\n",
    "\n",
    "torch.manual_seed(345)\n",
    "\n",
    "class ElmanRNN_ER(nn.Module):\n",
    "    \"\"\" RNN with a the same 'A' tensor for all time steps. \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, nonlinearity, device):\n",
    "        super(ElmanRNN_ER, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = input_size[0]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          nonlinearity=nonlinearity,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          device=device)\n",
    "        self.fc = nn.Linear(hidden_size, 2, device=device)\n",
    "        self.A = nn.Parameter(torch.zeros((self.batch_size, hidden_size), device=device), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        if h0 is None:\n",
    "            h0 =  torch.zeros((self.batch_size, hidden_size), device=device)\n",
    "        out, h = self.rnn(x, h0 + self.A)\n",
    "        return self.fc(out), h\n",
    "\n",
    "\n",
    "class ElmanRNN_ER2(nn.Module):\n",
    "    \"\"\" RNN with an 'A' tensor for each time step. \n",
    "    \n",
    "        Based on:\n",
    "        https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, batch_size, seq_len, nonlinearity, device):\n",
    "        super(ElmanRNN_ER2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          nonlinearity=nonlinearity,\n",
    "                          num_layers=1,  # more layers would require another dimension in A\n",
    "                          batch_first=True,\n",
    "                          device=device)\n",
    "        self.fc = nn.Linear(hidden_size, 2, device=device)\n",
    "        scaling_factor = np.sqrt(batch_size * seq_len * hidden_size) ** (-1)\n",
    "        self.A = nn.Parameter(scaling_factor * torch.randn((batch_size, seq_len, hidden_size),\n",
    "                                                           device=device), requires_grad=True)\n",
    "        self.params = dict(self.rnn.named_parameters())\n",
    "\n",
    "    def forward(self, x, hx=None, batch_first=False):\n",
    "        if batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        # print(f\"x shape: {x.shape}\")\n",
    "        # print(f\"A shape: {self.A.shape}\")\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        # assert seq_len == self.A.shape[1], f\"Different input and A sizes: {seq_len}, {self.A.shape[1]}.\"\n",
    "        assert batch_size == self.A.shape[0], f\"Different input and A batch sizes: {batch_size}, {self.A.shape[0]}.\"\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size, device=device)\n",
    "        h_t_minus_1 = hx.clone()\n",
    "        h_t = hx.clone()\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            for layer in range(self.rnn.num_layers):\n",
    "                input_t = x[t] if layer == 0 else h_t[layer - 1]\n",
    "                h_t[layer] = torch.tanh(\n",
    "                    input_t @ self.params[f\"weight_ih_l{layer}\"].T\n",
    "                    + (h_t_minus_1[layer] + self.A[:, t]) @ self.params[f\"weight_hh_l{layer}\"].T\n",
    "                    + self.params[f\"bias_hh_l{layer}\"]\n",
    "                    + self.params[f\"bias_ih_l{layer}\"]\n",
    "                )\n",
    "            output.append(self.fc(h_t[-1].clone()))\n",
    "            h_t_minus_1 = h_t.clone()\n",
    "        output = torch.stack(output)\n",
    "        if batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        return output, h_t\n",
    "\n",
    "    def reshape_A_batches(self, new_batch_size, device):\n",
    "        \"\"\" Change the batch size for the 'A' tensor. \"\"\"\n",
    "        seq_len = self.A.shape[1]\n",
    "        hidden_size = self.A.shape[2]\n",
    "        scaling_factor = np.sqrt(new_batch_size * seq_len * hidden_size) ** (-1)\n",
    "        self.A = nn.Parameter(scaling_factor * torch.randn((new_batch_size, seq_len, hidden_size),\n",
    "                                                           device=device), requires_grad=True)\n",
    "\n",
    "\n",
    "# rnn = ElmanRNN_ER(input_size, hidden_size, nonlinearity, coordinates.size()[0], device)\n",
    "# rnn = ElmanRNN_ER2(input_size, hidden_size, coordinates.shape[0], coordinates.size()[1], nonlinearity, device)\n",
    "# rnn.reshape_A_batches(4, device)\n",
    "\n",
    "# optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "# loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4461d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop with all training examples in a single batch (non-PVRNN)\n",
    "n_epochs = 1000 # number of epochs\n",
    "# xcoordinates = coordinates.unsqueeze(1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    out, _  = rnn(coordinates[:, :-1], batch_first=True)\n",
    "    error = loss(out, coordinates[:, 1:])\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Error at epoch {epoch} = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8178f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop breaking the data into two batches of 2 elements each (non-PVRNN)\n",
    "n_epochs = 1000\n",
    "batch1 = coordinates[0:2]\n",
    "batch2 = coordinates[2:]\n",
    "rnn.reshape_A_batches(2, device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # batch 1\n",
    "    optim.zero_grad()\n",
    "    out, _  = rnn(batch1[:, :-1], batch_first=True)\n",
    "    error = loss(out, batch1[:, 1:])\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Error at batch 1 epoch {epoch} = {error}\")\n",
    "    # batch 2 \n",
    "    optim.zero_grad()\n",
    "    out, _  = rnn(batch2[:, :-1], batch_first=True)\n",
    "    error = loss(out, batch2[:, 1:])\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Error at batch 2 epoch {epoch} = {error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bade50",
   "metadata": {},
   "source": [
    "---\n",
    "### The Elman PVRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e57f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, the optimizer, and the loss function\n",
    "input_size = 2  # this shouldn't change\n",
    "hidden_size = 256\n",
    "nonlinearity = 'tanh'  # tanh or relu\n",
    "lr = 3e-4  # learning rate\n",
    "all_generated = []\n",
    "all_generated_er = []\n",
    "\n",
    "torch.manual_seed(345)\n",
    "\n",
    "class ElmanPVRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, seq_len, nonlinearity, device):\n",
    "        super(ElmanPVRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          nonlinearity=nonlinearity,\n",
    "                          num_layers=1,  # more layers would require another dimension in A\n",
    "                          batch_first=True,\n",
    "                          device=device)\n",
    "        self.fc = nn.Linear(hidden_size, 2, device=device)\n",
    "        self.prior_decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size, device=device),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, 2 * hidden_size, device=device)\n",
    "            )\n",
    "        scaling_factor = np.sqrt(batch_size * seq_len * hidden_size) ** (-1)\n",
    "        self.A_mu = nn.Parameter(scaling_factor * torch.randn((batch_size, seq_len, hidden_size),\n",
    "                                                           device=device), requires_grad=True)\n",
    "        self.A_logvar = nn.Parameter(scaling_factor * torch.randn((batch_size, seq_len, hidden_size),\n",
    "                                                           device=device), requires_grad=True)\n",
    "        self.zh_l = nn.Linear(hidden_size, hidden_size, device=device)\n",
    "        self.params = dict(self.rnn.named_parameters())\n",
    "\n",
    "    def forward(self, x, hx=None, A_mu=None, batch_first=False, use_prior=False, deterministic=False):\n",
    "        \"\"\" Predict a sequence with Elman PV-RNN.\n",
    "        \n",
    "        :param x: input sequence, shaped (seq_len, N, seq_dim) or (N, seq_len, seq_dim) \n",
    "        :type x: torch.tensor\n",
    "        :hx: initial hidden state (n_layers, N, hidden_size)\n",
    "        :type hx: torch.tensor\n",
    "        :param A_mu: The posterior A_mu tensor to use. `use_prior` overrides this. \n",
    "        :type A_mu: torch.tensor, shape (N, seq_len, seq_dim).\n",
    "        :param batch_first: if True, x has shape (N, seq_len, seq_dim)\n",
    "        :type batch_first: bool\n",
    "        :param use_prior: if True, use the z value from the prior decoder\n",
    "        :type use_prior: bool\n",
    "        :param deterministic: if True, z will be equal to mu when sampling\n",
    "        :returns: predicted sequence, final hidden state, sequence of mu and logvar priors\n",
    "        :rtype: 4-tuple\n",
    "        \"\"\"\n",
    "        if batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        # print(f\"x shape: {x.shape}\")\n",
    "        # print(f\"A shape: {self.A.shape}\")\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        # assert seq_len == self.A.shape[1], f\"Different input and A sizes: {seq_len}, {self.A.shape[1]}.\"\n",
    "        assert batch_size == self.A_mu.shape[0], f\"Different input and A batch sizes: {batch_size}, {self.A_mu.shape[0]}.\"\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size, device=device)\n",
    "        h_t_minus_1 = hx.clone()\n",
    "        h_t = hx.clone()\n",
    "        output = []\n",
    "        mu_priors = []\n",
    "        logvar_priors = []\n",
    "        # print(f\"seq_len = {seq_len}!!!!!!!!!!!!!!!!!!!\")\n",
    "        for t in range(seq_len):\n",
    "            mu_prior_t, logvar_prior_t = torch.tensor_split(self.prior_decoder(h_t[0].clone().detach()), 2, dim=1) # 2*(batch_size, hidden size)\n",
    "\n",
    "            if use_prior:\n",
    "                z_t = self.sample_z_prior(mu_prior_t, logvar_prior_t, deterministic=deterministic).unsqueeze(0)\n",
    "            elif A_mu is None:\n",
    "                z_t = self.sample_z_post(t, deterministic=deterministic).unsqueeze(0)\n",
    "            else:\n",
    "                z_t = A_mu[:, t]\n",
    "            for layer in range(self.rnn.num_layers):\n",
    "                input_t = x[t] if layer == 0 else h_t[layer - 1]\n",
    "                h_t[layer] = torch.tanh(\n",
    "                    input_t @ self.params[f\"weight_ih_l{layer}\"].T\n",
    "                    + h_t_minus_1[layer] @ self.params[f\"weight_hh_l{layer}\"].T\n",
    "                    + self.params[f\"bias_hh_l{layer}\"]\n",
    "                    + self.params[f\"bias_ih_l{layer}\"]\n",
    "                    + self.zh_l(z_t)\n",
    "                )\n",
    "            output.append(self.fc(h_t[-1].clone()))\n",
    "            mu_priors.append(mu_prior_t.clone())\n",
    "            logvar_priors.append(logvar_prior_t.clone())\n",
    "            h_t_minus_1 = h_t.clone()\n",
    "        output = torch.stack(output)\n",
    "        if batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        mu_priors = torch.stack(mu_priors, dim=1)\n",
    "        logvar_priors = torch.stack(logvar_priors, dim=1)\n",
    "        return output, h_t, mu_priors, logvar_priors\n",
    "\n",
    "    def sample_z_post(self, step, deterministic=False):\n",
    "        if deterministic:\n",
    "            z_post = self.A_mu[:, step]\n",
    "        else:\n",
    "            z_post = self.A_mu[:, step] + (\n",
    "                0.5 * torch.exp(self.A_logvar[:, step]) *\n",
    "                torch.randn_like(self.A_mu[:, step]))\n",
    "        return z_post\n",
    "\n",
    "    def sample_z_prior(self, mu_prior_t, logvar_prior_t, deterministic=False):\n",
    "        if deterministic:\n",
    "            z_prior = mu_prior_t\n",
    "        else:\n",
    "            z_prior = mu_prior_t + (\n",
    "                0.5 * torch.randn_like(mu_prior_t) *\n",
    "                torch.exp(logvar_prior_t))\n",
    "        return z_prior\n",
    "\n",
    "    def reshape_A_batches(self, new_batch_size, device, keep_norm=False):\n",
    "        \"\"\" Change the batch size for the 'A' tensor. \n",
    "        \n",
    "            WARNING: this will disconnect the optimizer from the new\n",
    "            A_mu and A_logvar tensors. Make sure to update the optimiezer.\n",
    "        \"\"\"\n",
    "        seq_len = self.A_mu.shape[1]\n",
    "        hidden_size = self.A_mu.shape[2]\n",
    "        if keep_norm:\n",
    "            mu_norm = self.A_mu.norm()\n",
    "            logvar_norm =self.A_logvar.norm()\n",
    "        else:\n",
    "            mu_norm = 1.0\n",
    "            logvar_norm = 1.0\n",
    "        scaling_factor_mu = mu_norm * np.sqrt(new_batch_size * seq_len * hidden_size) ** (-1)\n",
    "        scaling_factor_logvar = logvar_norm * np.sqrt(new_batch_size * seq_len * hidden_size) ** (-1)\n",
    "        self.A_mu = nn.Parameter(scaling_factor_mu * torch.randn((new_batch_size, seq_len, hidden_size),\n",
    "                                                           device=device), requires_grad=True)\n",
    "        self.A_logvar = nn.Parameter(scaling_factor_logvar * torch.randn((new_batch_size, seq_len, hidden_size),\n",
    "                                                           device=device), requires_grad=True)\n",
    "\n",
    "    def set_A_mu(self, A_mu_values):\n",
    "        \"\"\" Replace the values of A_mu. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.A_mu.copy_(A_mu_values)\n",
    "\n",
    "\n",
    "def GaussianKLD(mu1, logvar1, mu2, logvar2, *, \n",
    "                logvar_min=-50.0, logvar_max=80.0, eps=1e-8, compress_thresh=None):\n",
    "    \"\"\" KL divergence between two Gaussian distributions.\n",
    "\n",
    "        :param mu1: mean of the posterior for all time steps\n",
    "        :type mu1: torch.Tensor shaped (N, T, hidden_size)\n",
    "        :param logvar1: log variance of the posterior for all time steps\n",
    "        :type logvar1: torch.Tensor shaped (N, T, hidden_size)\n",
    "        :param mu2: mean of the prior for all time steps\n",
    "        :type mu2: torch.Tensor shaped (N, T, hidden_size)\n",
    "        :param logvar2: log variance of the prior for all time steps\n",
    "        :type logvar2: torch.Tensor shaped (N, T, hidden_size)\n",
    "        :param compress_thresh: values beyond this will be \"compressed\" by the log function\n",
    "        :type compress_thresh: float\n",
    "        :returns: sum of all N * T * hidden_size KL divergences\n",
    "        :rtype: torch.Tensor shaped (1,)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ensure same dtype/device (cheap no-ops if already matching)\n",
    "    mu2       = mu2.to(dtype=mu1.dtype, device=mu1.device)\n",
    "    logvar2   = logvar2.to(dtype=mu1.dtype, device=mu1.device)\n",
    "    # Clamp log-variances to avoid exp overflow/underflow\n",
    "    lv1 = torch.clamp(logvar1, min=logvar_min, max=logvar_max)\n",
    "    lv2 = torch.clamp(logvar2, min=logvar_min, max=logvar_max)\n",
    "    var1 = torch.exp(lv1)              # bounded by clamp\n",
    "    var2 = torch.exp(lv2) + eps        # eps guards divide-by-zero\n",
    "    diff = mu1 - mu2\n",
    "    # # KL per element\n",
    "    kl = 0.5 * (lv2 - lv1 + (var1 + diff * diff) / var2 - 1.0)\n",
    "    # # Sum over all elements -> scalar\n",
    "    kl_sum = kl.sum()\n",
    "    # Optional smooth compression of huge values (keeps graph/device)\n",
    "    if compress_thresh is not None:\n",
    "        kl_sum = torch.where(kl_sum < compress_thresh, kl_sum, kl_sum + torch.log(kl_sum + eps))\n",
    "    return kl_sum\n",
    "\n",
    "\n",
    "rnn = ElmanPVRNN(input_size, hidden_size, coordinates.shape[0], coordinates.size()[1], nonlinearity, device)\n",
    "# rnn.reshape_A_batches(4, device)\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "loss = nn.MSELoss(reduction='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19135a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for ElmanPVRNN, single batch\n",
    "n_epochs = 1000 # number of epochs\n",
    "deterministic = True\n",
    "shuffle = True\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_idxs = torch.randperm(4) if shuffle else torch.arange(4)\n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(coordinates[batch_idxs, :-1], batch_first=True, deterministic=deterministic)\n",
    "    # print(f\"mu_priors shape: {mu_priors.shape}\")\n",
    "    # print(f\"logvar_priors shape: {logvar_priors.shape}\")\n",
    "    pred_error = loss(out, coordinates[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b434e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_A_params_list = []\n",
    "for name, param in rnn.named_parameters():\n",
    "    if name not in ['A_mu', 'A_sigma']:\n",
    "        no_A_params_list.append(param)\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(no_A_params_list, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fa753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for ElmanPVRNN, single batch, Jacobian Trace Maximization\n",
    "from torch.func import jvp\n",
    "n_epochs = 1000 # number of epochs\n",
    "deterministic = True\n",
    "shuffle = True\n",
    "A_norm_thresh = 0.001\n",
    "num_probes = 4\n",
    "\n",
    "\n",
    "def loss_A(A):  # the gradient of A as a function of A\n",
    "    optim.zero_grad()\n",
    "    out, _, _, _ = rnn(coordinates[:, :-1], A_mu=A, batch_first=True, deterministic=True)\n",
    "    return loss(out, coordinates[:, 1:])\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_idxs = torch.randperm(4) if shuffle else torch.arange(4)\n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(coordinates[batch_idxs, :-1], batch_first=True, deterministic=deterministic)\n",
    "    # print(f\"mu_priors shape: {mu_priors.shape}\")\n",
    "    # print(f\"logvar_priors shape: {logvar_priors.shape}\")\n",
    "    pred_error = loss(out, coordinates[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")\n",
    "        print(f\"A_mu grad norm: {rnn.A_mu.grad.norm()}\")\n",
    "        print(f\"hh grad norm: {rnn.rnn.weight_hh_l0.grad.norm()}\")\n",
    "    \n",
    "    if rnn.A_mu.grad.norm() < A_norm_thresh:\n",
    "        A_mu = rnn.A_mu.clone()\n",
    "        trace_est = 0.0\n",
    "        optim.zero_grad()\n",
    "        for _ in range(num_probes):\n",
    "            z = torch.empty_like(A_mu).bernoulli_(0.5).mul(2).sub(1)\n",
    "            _, Jz = jvp(loss_A, (A_mu,), (z,) )\n",
    "            trace_est = trace_est + (Jz * z).sum()\n",
    "        # trace_est = -trace_est / (num_probes * A_mu.shape[1])  # invert sign to maximize\n",
    "        trace_est = -trace_est / num_probes\n",
    "        print(f\"----------------->> Epoch {epoch}. Trace est: {-trace_est}\")\n",
    "        trace_est.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(rnn.A_mu - init_A).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5388bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for ElmanPVRNN, two batches\n",
    "n_epochs = 1000\n",
    "deterministic = True\n",
    "\n",
    "batch1 = coordinates[0:2]\n",
    "batch2 = coordinates[2:]\n",
    "rnn.reshape_A_batches(2, device, keep_norm=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # batch 1\n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(batch1[:, :-1], batch_first=True, deterministic=deterministic)\n",
    "    pred_error = loss(out, batch1[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Batch1 error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")\n",
    "\n",
    "    # batch 2 \n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(batch2[:, :-1], batch_first=True, deterministic=deterministic)\n",
    "    pred_error = loss(out, batch2[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Batch2 error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")\n",
    "        print(f\"A_mu grad norm: {rnn.A_mu.grad.norm()}\")\n",
    "        print(f\"hh grad norm: {rnn.rnn.weight_hh_l0.grad.norm()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585396af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for ElmanPVRNN, two batches, Jacobian trace maximization\n",
    "from torch.func import jvp\n",
    "\n",
    "n_epochs = 1000\n",
    "deterministic = True\n",
    "A_norm_thresh = 0.001\n",
    "num_probes = 4\n",
    "\n",
    "batch1 = coordinates[0:2]\n",
    "batch2 = coordinates[2:]\n",
    "rnn.reshape_A_batches(2, device, keep_norm=True)\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "def loss_A1(A):  # the gradient of A as a function of A\n",
    "    # rnn.set_A_mu(A)  # doesn't work with jvp\n",
    "    optim.zero_grad()\n",
    "    out, _, _, _ = rnn(batch1[:, :-1], A_mu=A, batch_first=True, deterministic=True)\n",
    "    return loss(out, batch1[:, 1:])\n",
    "\n",
    "def loss_A2(A):  # the gradient of A as a function of A\n",
    "    # rnn.set_A_mu(A)  # doesn't work with jvp\n",
    "    optim.zero_grad()\n",
    "    out, _, _, _ = rnn(batch2[:, :-1], A_mu=A, batch_first=True, deterministic=True)\n",
    "    return loss(out, batch2[:, 1:])\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "    # batch 1\n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(batch1[:, :-1], batch_first=True, deterministic=deterministic)\n",
    "    pred_error = loss(out, batch1[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Batch1 error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")\n",
    "        print(f\"A_mu grad norm: {rnn.A_mu.grad.norm()}\")\n",
    "    \n",
    "    if rnn.A_mu.grad.norm() < A_norm_thresh:\n",
    "        A_mu = rnn.A_mu.clone()\n",
    "        trace_est = 0.0\n",
    "        optim.zero_grad()\n",
    "        for _ in range(num_probes):\n",
    "            z = torch.empty_like(A_mu).bernoulli_(0.5).mul(2).sub(1)\n",
    "            _, Jz = jvp(loss_A1, (A_mu,), (z,) )\n",
    "            trace_est = trace_est + (Jz * z).sum()\n",
    "        # trace_est = -trace_est / (num_probes * A_mu.shape[1])  # invert sign to maximize\n",
    "        trace_est = -trace_est / num_probes\n",
    "        print(f\"----------------->> Epoch {epoch}. Trace est: {-trace_est}\")\n",
    "        trace_est.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # batch 2 \n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(batch2[:, :-1], batch_first=True, deterministic=deterministic)\n",
    "    pred_error = loss(out, batch2[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Batch2 error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")\n",
    "        print(f\"A_mu grad norm: {rnn.A_mu.grad.norm()}\")\n",
    "        print(f\"hh grad norm: {rnn.rnn.weight_hh_l0.grad.norm()}\")\n",
    "\n",
    "    if rnn.A_mu.grad.norm() < A_norm_thresh:\n",
    "        A_mu = rnn.A_mu.clone()\n",
    "        trace_est = 0.0\n",
    "        optim.zero_grad()\n",
    "        for _ in range(num_probes):\n",
    "            z = torch.empty_like(A_mu).bernoulli_(0.5).mul(2).sub(1)\n",
    "            _, Jz = jvp(loss_A2, (A_mu,), (z,) )\n",
    "            trace_est = trace_est + (Jz * z).sum()\n",
    "        # trace_est = -trace_est / (num_probes * A_mu.shape[1])  # invert sign to maximize\n",
    "        trace_est = -trace_est / num_probes\n",
    "        print(f\"------------------------>>  Epoch {epoch}. Trace est: {-trace_est}\")\n",
    "        trace_est.backward()\n",
    "        optim.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8670818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for ElmanPVRNN, two batches, Error Regression, Jacobian trace maximization\n",
    "from torch.func import jvp\n",
    "\n",
    "n_epochs = 5000\n",
    "deterministic = False\n",
    "A_norm_thresh = 0.05\n",
    "num_probes = 4\n",
    "er_steps = 6\n",
    "er_window = 5\n",
    "er_interval =20 \n",
    "er_stride = 8 * er_window\n",
    "bidirectional_er = True\n",
    "\n",
    "batch1 = coordinates[0:2]\n",
    "batch2 = coordinates[2:]\n",
    "rnn.reshape_A_batches(2, device, keep_norm=True)\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "seq_len = coordinates.shape[1]\n",
    "\n",
    "\n",
    "def loss_A1(A):  # the gradient of A as a function of A\n",
    "    # rnn.set_A_mu(A)  # doesn't work with jvp\n",
    "    optim.zero_grad()\n",
    "    out, _, _, _ = rnn(batch1[:, :-1], A_mu=A, batch_first=True, deterministic=True)\n",
    "    return loss(out, batch1[:, 1:])\n",
    "\n",
    "\n",
    "def loss_A2(A):  # the gradient of A as a function of A\n",
    "    # rnn.set_A_mu(A)  # doesn't work with jvp\n",
    "    optim.zero_grad()\n",
    "    out, _, _, _ = rnn(batch2[:, :-1], A_mu=A, batch_first=True, deterministic=True)\n",
    "    return loss(out, batch2[:, 1:])\n",
    "\n",
    "\n",
    "def pointwise_error_regression(model, optim, init_idx, h, er_steps, er_window, coordinates):\n",
    "    \"\"\" Run error regression one time step at a time. \n",
    "\n",
    "        :param model: the model to run\n",
    "        :type model: torch.Module\n",
    "        :param optim: the parameter optimizer\n",
    "        :type optim: torch.optim\n",
    "        :param init_idx: index of the first coordinate to use.\n",
    "        :type init_idx: int\n",
    "        :param h: hidden state, shape (1, batch_size, hidden_size)\n",
    "        :type h: torch.tensor\n",
    "        :param er_steps: number of ER steps\n",
    "        :type er_steps: int\n",
    "        :param er_window: size of ER window\n",
    "        :type er_window: int\n",
    "        :param coordinates: all coordinates, shape (batch_size, seq_len, 2)\n",
    "        :type coordinates: torch.tensor\n",
    "    \"\"\"\n",
    "    batch_size = coordinates.shape[0]\n",
    "    seq_len = coordinates.shape[1]\n",
    "    gen_coordinates = torch.zeros((batch_size, seq_len, 2), device=device)\n",
    "    coords = coordinates[:, init_idx:init_idx+1].clone()\n",
    "    for point_idx in range(init_idx, min(seq_len, init_idx + er_window)):\n",
    "        for er_idx in range(er_steps):\n",
    "            optim.zero_grad()\n",
    "            h = h.detach()  # so h doesn't point to the t-1 computational graph\n",
    "            if er_idx == 0:\n",
    "                use_prior = True\n",
    "            else:\n",
    "                use_prior = False\n",
    "            new_coords, h, _, _ = model(coords, h, batch_first=True, use_prior=use_prior, deterministic=deterministic)\n",
    "            gen_coordinates[:, point_idx, :] = new_coords.squeeze()\n",
    "            init_idx = point_idx - max(0, point_idx - er_window)\n",
    "            error = (coordinates[:, init_idx:point_idx + 1] - \n",
    "                     gen_coordinates[:, init_idx:point_idx + 1]).norm()\n",
    "            error.backward()\n",
    "            gen_coordinates = gen_coordinates.detach()\n",
    "            # print(f\"pass {er_idx}\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in [\"A_mu\", \"A_logvar\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.detach()\n",
    "                        param.grad.zero_()\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "def error_regression(model, optim, init_idx, h, er_steps, er_window, coordinates, bidirectional=True):\n",
    "    \"\"\" Run error regression. \n",
    "\n",
    "        :param model: the model to run\n",
    "        :type model: torch.Module\n",
    "        :param optim: the parameter optimizer\n",
    "        :type optim: torch.optim\n",
    "        :param init_idx: index of the first coordinate to use.\n",
    "        :type init_idx: int\n",
    "        :param h: hidden state, shape (1, batch_size, hidden_size)\n",
    "        :type h: torch.tensor\n",
    "        :param er_steps: number of ER steps\n",
    "        :type er_steps: int\n",
    "        :param er_window: size of ER window\n",
    "        :type er_window: int\n",
    "        :param coordinates: all coordinates, shape (batch_size, seq_len, 2)\n",
    "        :type coordinates: torch.tensor\n",
    "    \"\"\"\n",
    "    # batch_size = coordinates.shape[0]\n",
    "    seq_len = coordinates.shape[1]\n",
    "    # gen_coordinates = torch.zeros((batch_size, seq_len, 2), device=device)\n",
    "    init_idx = max(0, init_idx - er_window)\n",
    "    if bidirectional:\n",
    "        end_idx = min(seq_len, init_idx + er_window)\n",
    "    else:\n",
    "        end_idx = min(seq_len, init_idx + 1)\n",
    "    coords = coordinates[:, init_idx:end_idx].clone()\n",
    "    for er_idx in range(er_steps):\n",
    "        optim.zero_grad()\n",
    "        h = h.detach()  # so h doesn't point to the t-1 computational graph\n",
    "        if er_idx == 0:\n",
    "            use_prior = True\n",
    "        else:\n",
    "            use_prior = False\n",
    "        new_coords, h, _, _ = model(coords, h, batch_first=True, use_prior=use_prior, deterministic=deterministic)\n",
    "        # gen_coordinates[:, init_idx:end_idx, :] = new_coords.squeeze()\n",
    "        error = (coordinates[:, init_idx:end_idx] - new_coords).norm()\n",
    "        error.backward()\n",
    "        # gen_coordinates = gen_coordinates.detach()\n",
    "        # print(f\"pass {er_idx}\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in [\"A_mu\", \"A_logvar\"]:\n",
    "                continue\n",
    "            else:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.detach()\n",
    "                    param.grad.zero_()\n",
    "        optim.step()\n",
    "\n",
    "#------------------------------ the training loop ------------------------------\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "    # batch 1\n",
    "    if epoch % er_interval == 0 and epoch > 20:\n",
    "        h = torch.zeros((1, batch1.shape[0], hidden_size), device=device)\n",
    "        for coord_idx in range(0, seq_len - er_window, er_stride):\n",
    "            error_regression(rnn, optim, coord_idx, h, er_steps, er_window, batch1)\n",
    "            print('.', end='')\n",
    "        print(\" \")\n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(batch1[:, :-1], batch_first=True, deterministic=deterministic)\n",
    "    pred_error = loss(out, batch1[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Batch1 error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")\n",
    "        print(f\"A_mu grad norm: {rnn.A_mu.grad.norm()}\")\n",
    "        print(f\"A_mu norm: {rnn.A_mu.norm()}\")\n",
    "    \n",
    "    # if rnn.A_mu.grad.norm() < A_norm_thresh:\n",
    "    #     A_mu = rnn.A_mu.clone()\n",
    "    #     trace_est = 0.0\n",
    "    #     optim.zero_grad()\n",
    "    #     for _ in range(num_probes):\n",
    "    #         z = torch.empty_like(A_mu).bernoulli_(0.5).mul(2).sub(1)\n",
    "    #         _, Jz = jvp(loss_A1, (A_mu,), (z,) )\n",
    "    #         trace_est = trace_est + (Jz * z).sum()\n",
    "    #     trace_est = -trace_est / num_probes\n",
    "    #     print(f\"------------------------>>  Epoch {epoch}. Trace est: {-trace_est}\")\n",
    "    #     trace_est.backward()\n",
    "    #     optim.step()\n",
    "\n",
    "    # batch 2 \n",
    "    if epoch % er_interval == 0 and epoch > 20:\n",
    "        h = torch.zeros((1, batch2.shape[0], hidden_size), device=device)\n",
    "        for coord_idx in range(0, seq_len - er_window, er_stride):\n",
    "            error_regression(rnn, optim, coord_idx, h, er_steps, er_window, batch2)\n",
    "            print('*', end='')\n",
    "        print(\" \")\n",
    "    optim.zero_grad()\n",
    "    out, _, mu_priors, logvar_priors  = rnn(batch2[:, :-1], batch_first=True, deterministic=deterministic)\n",
    "    pred_error = loss(out, batch2[:, 1:])\n",
    "    KL_div = GaussianKLD(rnn.A_mu[:, :-1], rnn.A_logvar[:, :-1], mu_priors, logvar_priors)\n",
    "    error = pred_error + 0.0001 * KL_div\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Batch2 error at epoch {epoch} = {error}. pred error: {pred_error}, KL div: {KL_div}\")\n",
    "        print(f\"A_mu grad norm: {rnn.A_mu.grad.norm()}\")\n",
    "        print(f\"A_mu norm: {rnn.A_mu.norm()}\")\n",
    "        print(f\"hh grad norm: {rnn.rnn.weight_hh_l0.grad.norm()}\")\n",
    "\n",
    "    # A_mu_norm = rnn.A_mu.grad.norm()\n",
    "    # if  A_mu_norm < A_norm_thresh or A_mu_norm > 20:\n",
    "    #     trace_factor = -0.5 if A_mu_norm < A_norm_thresh else 1.0\n",
    "    #     A_mu = rnn.A_mu.clone()\n",
    "    #     trace_est = 0.0\n",
    "    #     optim.zero_grad()\n",
    "    #     for _ in range(num_probes):\n",
    "    #         z = torch.empty_like(A_mu).bernoulli_(0.5).mul(2).sub(1)\n",
    "    #         _, Jz = jvp(loss_A2, (A_mu,), (z,) )\n",
    "    #         trace_est = trace_est + (Jz * z).sum()\n",
    "    #     trace_est = trace_factor * trace_est / num_probes\n",
    "    #     print(f\"-------------->>  Epoch {epoch}. Trace est: {trace_est}. trace factor: {trace_factor}. A_mu norm: {A_mu_norm}\")\n",
    "    #     trace_est.backward()\n",
    "    #     optim.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e67c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.zero_grad(set_to_none=False)\n",
    "print(rnn.A_mu.grad.norm())\n",
    "print(rnn.rnn.weight_hh_l0.grad.norm())\n",
    "# should be True:\n",
    "print(any(p is rnn.A_mu for g in optim.param_groups for p in g[\"params\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353ac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), 'pvrnn_2batches_ER_JTM_nonoise_epoch5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643af6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved data (2 batches)\n",
    "rnn.reshape_A_batches(2, device, keep_norm=True)\n",
    "rnn.load_state_dict(torch.load('pvrnn_2batches_epoch10000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distances between sections of A\n",
    "distances = []\n",
    "angles = []\n",
    "batch_size = A_clone.shape[0]\n",
    "for i in range(batch_size):\n",
    "    for j in range(i+1, batch_size):\n",
    "        dist = (A_clone[i] - A_clone[j]).norm()\n",
    "        Ai = A_clone[i] / A_clone[i].norm()\n",
    "        Aj = A_clone[j] / A_clone[j].norm()\n",
    "        angle = np.arccos((Ai * Aj).sum().detach().cpu())\n",
    "        print(f\"i = {i}, j = {j}, dist = {dist}, angle = {angle}\")\n",
    "        distances.append(dist)\n",
    "        angles.append(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.reshape_A_batches(4, device)\n",
    "with torch.no_grad():\n",
    "    rnn.A *= A_clone.norm()\n",
    "\n",
    "print(rnn.A.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subten in A_clone:\n",
    "    print(subten.norm())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f733a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_er = []\n",
    "all_generated = []\n",
    "all_generated_er.append(gen_coordinates_er)\n",
    "all_generated.append(gen_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a trajectory autoregressively (non-PVRNN)\n",
    "n_points = coordinates.shape[1]\n",
    "# coordinate = torch.tensor([-1, 1], dtype=torch.float32, device=device)\n",
    "init_coords = coordinates[:,0:1]\n",
    "print(f\"init coords shape : {init_coords.shape}\")\n",
    "er_steps = 30\n",
    "er_window = 15\n",
    "\n",
    "# rnn.reshape_A_batches(2, device)\n",
    "\n",
    "def generate_trajectory(init_coordinates, n_points):\n",
    "    \"\"\" Standard autoregressive trajectory generation. \"\"\"\n",
    "    batch_size = init_coordinates.shape[0]\n",
    "    gen_coordinates = np.empty((batch_size, n_points, 2))\n",
    "    coords = init_coordinates.clone()\n",
    "    h = torch.zeros((1, batch_size, hidden_size), device=device)\n",
    "    with torch.no_grad():\n",
    "        for point_idx in range(n_points):\n",
    "            # print(f\"coordinate shape before call: {coords.shape}\")\n",
    "            coords, h = rnn(coords, h, batch_first=True)\n",
    "            # print(f\"coordinate shape after call: {coordinate.shape}\")\n",
    "            gen_coordinates[:, point_idx, :] = coords.detach().squeeze().cpu().numpy()\n",
    "    return gen_coordinates\n",
    "\n",
    "\n",
    "def generate_trajectory_ER(init_coordinates, n_points, er_steps, er_window, all_coordinates):\n",
    "    \"\"\" Autoregressive trajectory generated with error regression. \"\"\"\n",
    "    batch_size = init_coordinates.shape[0]\n",
    "    gen_coordinates = torch.zeros((batch_size, n_points, 2), device=device)\n",
    "    coords = init_coordinates.clone()\n",
    "    h = torch.zeros((1, batch_size, hidden_size), device=device)\n",
    "    for point_idx in range(n_points):\n",
    "        for er_idx in range(er_steps):\n",
    "            optim.zero_grad()\n",
    "            h = h.detach()  # so h doesn't point to the t-1 computational graph\n",
    "            new_coords, h = rnn(coords, h, batch_first=True)\n",
    "            gen_coordinates[:, point_idx, :] = new_coords.squeeze()\n",
    "            init_idx = point_idx - max(0, point_idx - er_window)\n",
    "            error = (all_coordinates[:, init_idx:point_idx + 1] - \n",
    "                     gen_coordinates[:, init_idx:point_idx + 1]).norm()\n",
    "            error.backward()\n",
    "            gen_coordinates = gen_coordinates.detach()\n",
    "            # print(f\"pass {er_idx}\")\n",
    "            for name, param in rnn.named_parameters():\n",
    "                if name == \"A\":\n",
    "                    continue\n",
    "                else:\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.detach()\n",
    "                        param.grad.zero_()\n",
    "            optim.step()\n",
    "            if point_idx % 100 == 0:\n",
    "                print(f\"Point {point_idx} ER step {er_idx} error: {error} \")\n",
    "        coords = new_coords.detach()\n",
    "    return gen_coordinates.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "gen_coordinates = generate_trajectory(init_coords, n_points)\n",
    "gen_coordinates_er = generate_trajectory_ER(init_coords, n_points, er_steps, er_window, coordinates[:])\n",
    "all_generated_er.append(gen_coordinates_er)\n",
    "all_generated.append(gen_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546edd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a trajectory autoregressively (PVRNN)\n",
    "n_points = 600 #coordinates.shape[1]\n",
    "# coordinate = torch.tensor([-1, 1], dtype=torch.float32, device=device)\n",
    "init_coords = coordinates[:,0:1]\n",
    "print(f\"init coords shape : {init_coords.shape}\")\n",
    "er_steps = 100\n",
    "er_window = 40\n",
    "deterministic = True\n",
    "# all_generated = []\n",
    "# all_generated_er = []\n",
    "\n",
    "rnn.reshape_A_batches(4, device)\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "def generate_trajectory(init_coordinates, n_points):\n",
    "    \"\"\" Standard autoregressive trajectory generation. \"\"\"\n",
    "    batch_size = init_coordinates.shape[0]\n",
    "    gen_coordinates = np.empty((batch_size, n_points, 2))\n",
    "    coords = init_coordinates.clone()\n",
    "    with torch.no_grad():\n",
    "        for point_idx in range(n_points):\n",
    "            # print(f\"coordinate shape before call: {coords.shape}\")\n",
    "            coords, h, _, _ = rnn(coords, batch_first=True, deterministic=deterministic)\n",
    "            # print(f\"coordinate shape after call: {coordinate.shape}\")\n",
    "            gen_coordinates[:, point_idx, :] = coords.detach().squeeze().cpu().numpy()\n",
    "    return gen_coordinates\n",
    "\n",
    "\n",
    "def generate_trajectory_ER(init_coordinates, n_points, er_steps, er_window, all_coordinates):\n",
    "    \"\"\" Autoregressive trajectory generated with error regression. \"\"\"\n",
    "    batch_size = init_coordinates.shape[0]\n",
    "    gen_coordinates = torch.zeros((batch_size, n_points, 2), device=device)\n",
    "    coords = init_coordinates.clone()\n",
    "    h = torch.zeros((1, batch_size, hidden_size), device=device)\n",
    "    for point_idx in range(n_points):\n",
    "        for er_idx in range(er_steps):\n",
    "            optim.zero_grad()\n",
    "            h = h.detach()  # so h doesn't point to the t-1 computational graph\n",
    "            if er_idx == 0:\n",
    "                use_prior = True\n",
    "            else:\n",
    "                use_prior = False\n",
    "            new_coords, h, _, _ = rnn(coords, h, batch_first=True, use_prior=use_prior, deterministic=deterministic)\n",
    "            gen_coordinates[:, point_idx, :] = new_coords.squeeze()\n",
    "            init_idx = point_idx - max(0, point_idx - er_window)\n",
    "            error = (all_coordinates[:, init_idx:point_idx + 1] - \n",
    "                     gen_coordinates[:, init_idx:point_idx + 1]).norm()\n",
    "            error.backward()\n",
    "            gen_coordinates = gen_coordinates.detach()\n",
    "            # print(f\"pass {er_idx}\")\n",
    "            for name, param in rnn.named_parameters():\n",
    "                if name in [\"A_mu\", \"A_logvar\"]:\n",
    "                    print('.', end='')\n",
    "                    continue\n",
    "                else:\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.detach()\n",
    "                        param.grad.zero_()\n",
    "            optim.step()\n",
    "            if point_idx % 100 == 0:\n",
    "                print(f\"Point {point_idx} ER step {er_idx} error: {error} \")\n",
    "\n",
    "        # A_mu_norm = rnn.A_mu.grad.norm()\n",
    "        # if  A_mu_norm < A_norm_thresh: # or A_mu_norm > 20:\n",
    "        #     trace_factor = -0.5 if A_mu_norm < A_norm_thresh else 1.0\n",
    "        #     A_mu = rnn.A_mu.clone()\n",
    "        #     trace_est = 0.0\n",
    "        #     optim.zero_grad()\n",
    "        #     for _ in range(num_probes):\n",
    "        #         z = torch.empty_like(A_mu).bernoulli_(0.5).mul(2).sub(1)\n",
    "        #         _, Jz = jvp(loss_A2, (A_mu,), (z,) )\n",
    "        #         trace_est = trace_est + (Jz * z).sum()\n",
    "        #     trace_est = trace_factor * trace_est / num_probes\n",
    "        #     print(f\"-------------->>  Epoch {epoch}. Trace est: {trace_est}. trace factor: {trace_factor}. A_mu norm: {A_mu_norm}\")\n",
    "        #     trace_est.backward()\n",
    "        #     optim.step()\n",
    "            \n",
    "        coords = new_coords.detach()\n",
    "    return gen_coordinates.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "gen_coordinates = generate_trajectory(init_coords, n_points)\n",
    "gen_coordinates_er = generate_trajectory_ER(init_coords, n_points, er_steps, er_window, coordinates[:])\n",
    "all_generated_er.append(gen_coordinates_er)\n",
    "all_generated.append(gen_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1c08d-02d6-484d-99d2-fc19b9f1071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_idx = 0  # index of the example to plot\n",
    "\n",
    "n_traces = len(all_generated)\n",
    "\n",
    "if n_traces > 3:\n",
    "    n_rows = int(np.floor(np.sqrt(n_traces + 1)))\n",
    "    n_cols = int(np.ceil((n_traces+1) / n_rows))\n",
    "else:\n",
    "    n_cols = n_traces + 1 \n",
    "    n_rows = 1\n",
    "fig, axs = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(5.1*n_cols,4.6*n_rows))\n",
    "for ax in axs:\n",
    "    if type(ax) == np.ndarray:\n",
    "        for _ax in ax:\n",
    "            _ax.set_aspect('equal')\n",
    "    else:\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "original = coordinates.cpu().detach()\n",
    "if n_rows > 1:\n",
    "    ax0 = axs[0,0]\n",
    "else:\n",
    "    ax0 = axs[0]\n",
    "ax0.plot(original[ex_idx, :, 0], original[ex_idx, :, 1])\n",
    "ax0.set_title('original trace')\n",
    "for trace in range(n_traces):\n",
    "    gen_coordinates = all_generated[trace]\n",
    "    if n_rows == 1:\n",
    "        ax = axs[trace+1]\n",
    "    else:\n",
    "        row = int(np.floor((trace+1)/n_cols))\n",
    "        col = int(np.ceil((trace+1)%n_cols)) \n",
    "        ax = axs[row, col]\n",
    "    # ax.set_title(f'After {n_epochs*(trace+1)} epochs')\n",
    "    if trace == 0:\n",
    "        er_st = 10\n",
    "        er_win = 10\n",
    "    elif trace == 1:\n",
    "        er_st = 30\n",
    "        er_win = 15\n",
    "    else:\n",
    "        er_st = 100\n",
    "        er_win = 40\n",
    "    ax.set_title(f\"ER steps: {er_st}, ER window: {er_win}\")\n",
    "    ax.plot(gen_coordinates[ex_idx, :, 0], gen_coordinates[ex_idx, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traces = len(all_generated_er)\n",
    "\n",
    "if n_traces > 3:\n",
    "    n_rows = int(np.floor(np.sqrt(n_traces + 1)))\n",
    "    n_cols = int(np.ceil((n_traces+1) / n_rows))\n",
    "else:\n",
    "    n_cols = n_traces + 1 \n",
    "    n_rows = 1\n",
    "fig, axs = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(5.1*n_cols,4.6*n_rows))\n",
    "for ax in axs:\n",
    "    if type(ax) == np.ndarray:\n",
    "        for _ax in ax:\n",
    "            _ax.set_aspect('equal')\n",
    "    else:\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "original = coordinates.cpu().detach()\n",
    "if n_rows > 1:\n",
    "    ax0 = axs[0,0]\n",
    "else:\n",
    "    ax0 = axs[0]\n",
    "# ax0.plot(original[ex_idx+2, :, 0], original[ex_idx+2, :, 1])\n",
    "ax0.plot(original[ex_idx, :, 0], original[ex_idx, :, 1])\n",
    "ax0.set_title('original trace')\n",
    "for trace in range(n_traces):\n",
    "    gen_coordinates = all_generated_er[trace]\n",
    "    if n_rows == 1:\n",
    "        ax = axs[trace+1]\n",
    "    else:\n",
    "        row = int(np.floor((trace+1)/n_cols))\n",
    "        col = int(np.ceil((trace+1)%n_cols)) \n",
    "        ax = axs[row, col]\n",
    "    # ax.set_title(f'After {n_epochs*(trace+1)} epochs')\n",
    "    if trace == 0:\n",
    "        er_st = 10\n",
    "        er_win = 10\n",
    "    elif trace == 1:\n",
    "        er_st = 30\n",
    "        er_win = 15\n",
    "    else:\n",
    "        er_st = 100\n",
    "        er_win = 40\n",
    "    ax.set_title(f\"ER steps: {er_st}, ER window: {er_win}\")\n",
    "    ax.plot(gen_coordinates[ex_idx, :, 0], gen_coordinates[ex_idx, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_idx = 1\n",
    "\n",
    "n_traces = len(all_generated_er)\n",
    "\n",
    "if n_traces > 3:\n",
    "    n_rows = int(np.floor(np.sqrt(n_traces + 1)))\n",
    "    n_cols = int(np.ceil((n_traces+1) / n_rows))\n",
    "else:\n",
    "    n_cols = n_traces + 1 \n",
    "    n_rows = 1\n",
    "fig, axs = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(5.1*n_cols,4.6*n_rows))\n",
    "for ax in axs:\n",
    "    if type(ax) == np.ndarray:\n",
    "        for _ax in ax:\n",
    "            _ax.set_aspect('equal')\n",
    "    else:\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "original = coordinates.cpu().detach()\n",
    "if n_rows > 1:\n",
    "    ax0 = axs[0,0]\n",
    "else:\n",
    "    ax0 = axs[0]\n",
    "# ax0.plot(original[ex_idx+2, :, 0], original[ex_idx+2, :, 1])\n",
    "ax0.plot(original[ex_idx, :, 0], original[ex_idx, :, 1])\n",
    "ax0.set_title('original trace')\n",
    "for trace in range(n_traces):\n",
    "    gen_coordinates = all_generated_er[trace]\n",
    "    if n_rows == 1:\n",
    "        ax = axs[trace+1]\n",
    "    else:\n",
    "        row = int(np.floor((trace+1)/n_cols))\n",
    "        col = int(np.ceil((trace+1)%n_cols)) \n",
    "        ax = axs[row, col]\n",
    "    # ax.set_title(f'After {n_epochs*(trace+1)} epochs')\n",
    "    if trace == 0:\n",
    "        er_st = 10\n",
    "        er_win = 10\n",
    "    elif trace == 1:\n",
    "        er_st = 30\n",
    "        er_win = 15\n",
    "    else:\n",
    "        er_st = 100\n",
    "        er_win = 40\n",
    "    ax.set_title(f\"ER steps: {er_st}, ER window: {er_win}\")\n",
    "    ax.plot(gen_coordinates[ex_idx, :, 0], gen_coordinates[ex_idx, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_idx = 2\n",
    "\n",
    "n_traces = len(all_generated_er)\n",
    "\n",
    "if n_traces > 3:\n",
    "    n_rows = int(np.floor(np.sqrt(n_traces + 1)))\n",
    "    n_cols = int(np.ceil((n_traces+1) / n_rows))\n",
    "else:\n",
    "    n_cols = n_traces + 1 \n",
    "    n_rows = 1\n",
    "fig, axs = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(5.1*n_cols,4.6*n_rows))\n",
    "for ax in axs:\n",
    "    if type(ax) == np.ndarray:\n",
    "        for _ax in ax:\n",
    "            _ax.set_aspect('equal')\n",
    "    else:\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "original = coordinates.cpu().detach()\n",
    "if n_rows > 1:\n",
    "    ax0 = axs[0,0]\n",
    "else:\n",
    "    ax0 = axs[0]\n",
    "ax0.plot(original[ex_idx, :, 0], original[ex_idx, :, 1])\n",
    "ax0.set_title('original trace')\n",
    "for trace in range(n_traces):\n",
    "    gen_coordinates = all_generated_er[trace]\n",
    "    if n_rows == 1:\n",
    "        ax = axs[trace+1]\n",
    "    else:\n",
    "        row = int(np.floor((trace+1)/n_cols))\n",
    "        col = int(np.ceil((trace+1)%n_cols)) \n",
    "        ax = axs[row, col]\n",
    "    # ax.set_title(f'After {n_epochs*(trace+1)} epochs')\n",
    "    if trace == 0:\n",
    "        er_st = 10\n",
    "        er_win = 10\n",
    "    elif trace == 1:\n",
    "        er_st = 30\n",
    "        er_win = 15\n",
    "    else:\n",
    "        er_st = 100\n",
    "        er_win = 40\n",
    "    ax.set_title(f\"ER steps: {er_st}, ER window: {er_win}\")\n",
    "    ax.plot(gen_coordinates[ex_idx, :, 0], gen_coordinates[ex_idx, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_idx = 3\n",
    "\n",
    "n_traces = len(all_generated_er)\n",
    "\n",
    "if n_traces > 3:\n",
    "    n_rows = int(np.floor(np.sqrt(n_traces + 1)))\n",
    "    n_cols = int(np.ceil((n_traces+1) / n_rows))\n",
    "else:\n",
    "    n_cols = n_traces + 1 \n",
    "    n_rows = 1\n",
    "fig, axs = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(5.1*n_cols,4.6*n_rows))\n",
    "for ax in axs:\n",
    "    if type(ax) == np.ndarray:\n",
    "        for _ax in ax:\n",
    "            _ax.set_aspect('equal')\n",
    "    else:\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "original = coordinates.cpu().detach()\n",
    "if n_rows > 1:\n",
    "    ax0 = axs[0,0]\n",
    "else:\n",
    "    ax0 = axs[0]\n",
    "ax0.plot(original[ex_idx, :, 0], original[ex_idx, :, 1])\n",
    "ax0.set_title('original trace')\n",
    "for trace in range(n_traces):\n",
    "    gen_coordinates = all_generated_er[trace]\n",
    "    if n_rows == 1:\n",
    "        ax = axs[trace+1]\n",
    "    else:\n",
    "        row = int(np.floor((trace+1)/n_cols))\n",
    "        col = int(np.ceil((trace+1)%n_cols)) \n",
    "        ax = axs[row, col]\n",
    "    # ax.set_title(f'After {n_epochs*(trace+1)} epochs')\n",
    "    if trace == 0:\n",
    "        er_st = 10\n",
    "        er_win = 10\n",
    "    elif trace == 1:\n",
    "        er_st = 30\n",
    "        er_win = 15\n",
    "    else:\n",
    "        er_st = 100\n",
    "        er_win = 40\n",
    "    ax.set_title(f\"ER steps: {er_st}, ER window: {er_win}\")\n",
    "    ax.plot(gen_coordinates[ex_idx, :, 0], gen_coordinates[ex_idx, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50359689-a23b-4258-a2d5-01afb066e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a direction field\n",
    "ex_idx = 0\n",
    "lim_x = 1.2 # largest magnitude of x\n",
    "lim_y = 1.2\n",
    "N_x = 3 # number of points in x dimension\n",
    "N_y = 3\n",
    "n_points = 400\n",
    "bs = rnn.A_mu.shape[0]\n",
    "#df_fig = plt.figure(figsize = (10,10))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "axs[0].set_xlim(-1.1*lim_x, 1.1*lim_x)\n",
    "axs[0].set_ylim(-1.1*lim_y, 1.1*lim_y)\n",
    "axs[0].axis('equal')\n",
    "for x in np.linspace(-lim_x, lim_x, N_x):\n",
    "    for y in np.linspace(-lim_y, lim_y, N_y):\n",
    "        #print(f\"x = {x}, y = {y}\")\n",
    "        axs[0].scatter([x], [y], color='tab:pink')\n",
    "        axs[1].scatter([x], [y], color='tab:pink')\n",
    "        coordinate = torch.tensor([x, y], dtype=torch.float32, device=device).unsqueeze(dim=0)\n",
    "        coordinate = coordinate.tile((bs, 1, 1))\n",
    "        trajectory1 = generate_trajectory(coordinate, n_points)\n",
    "        trajectory2 = generate_trajectory_ER(coordinate, n_points, 20, 10, coordinates[:bs])\n",
    "        axs[0].scatter([trajectory1[ex_idx, 0, 0]], [trajectory1[ex_idx, 0, 1]], color='tab:cyan')\n",
    "        axs[1].scatter([trajectory2[ex_idx, 0, 0]], [trajectory2[ex_idx, 0,1]], color='tab:cyan')\n",
    "        axs[0].plot(trajectory1[ex_idx, :, 0], trajectory1[ex_idx, :, 1])\n",
    "        axs[1].plot(trajectory2[ex_idx, :, 0], trajectory2[ex_idx, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_idx = 1\n",
    "\n",
    "# plot a direction field\n",
    "lim_x = 1.2 # largest magnitude of x\n",
    "lim_y = 1.2\n",
    "N_x = 3 # number of points in x dimension\n",
    "N_y = 3\n",
    "n_points = 400\n",
    "bs = rnn.A_mu.shape[0]\n",
    "#df_fig = plt.figure(figsize = (10,10))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "axs[0].set_xlim(-1.1*lim_x, 1.1*lim_x)\n",
    "axs[0].set_ylim(-1.1*lim_y, 1.1*lim_y)\n",
    "axs[0].axis('equal')\n",
    "for x in np.linspace(-lim_x, lim_x, N_x):\n",
    "    for y in np.linspace(-lim_y, lim_y, N_y):\n",
    "        #print(f\"x = {x}, y = {y}\")\n",
    "        axs[0].scatter([x], [y], color='tab:pink')\n",
    "        axs[1].scatter([x], [y], color='tab:pink')\n",
    "        coordinate = torch.tensor([x, y], dtype=torch.float32, device=device).unsqueeze(dim=0)\n",
    "        coordinate = coordinate.tile((bs, 1, 1))\n",
    "        trajectory1 = generate_trajectory(coordinate, n_points)\n",
    "        trajectory2 = generate_trajectory_ER(coordinate, n_points, 20, 10, coordinates[:bs])\n",
    "        axs[0].scatter([trajectory1[ex_idx, 0, 0]], [trajectory1[ex_idx, 0, 1]], color='tab:cyan')\n",
    "        axs[1].scatter([trajectory2[ex_idx, 0, 0]], [trajectory2[ex_idx, 0,1]], color='tab:cyan')\n",
    "        axs[0].plot(trajectory1[ex_idx, :, 0], trajectory1[ex_idx, :, 1])\n",
    "        axs[1].plot(trajectory2[ex_idx, :, 0], trajectory2[ex_idx, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c4afc-e5c7-4622-a389-ea7e1ab72cdd",
   "metadata": {},
   "source": [
    "## LSTM Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17026b4c-98db-4a45-80af-eb574c6e3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, but using LSTM\n",
    "# Create the network, the optimizer, and the loss function\n",
    "input_size = 2  # this shouldn't change\n",
    "hidden_size = 256\n",
    "lr = 3e-4  # learning rate\n",
    "all_generated = []\n",
    "\n",
    "torch.manual_seed(345)\n",
    "\n",
    "class LSTM_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            batch_first=True,\n",
    "                            device=device)\n",
    "        self.fc = nn.Linear(hidden_size, 2, device=device)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        if h0 is None:\n",
    "            h0 =  torch.zeros((1, hidden_size), device=device)\n",
    "        if c0 is None:\n",
    "            c0 =  torch.zeros((1, hidden_size), device=device)\n",
    "        out, (h, c) = self.lstm(x, (h0, c0))\n",
    "        return self.fc(out), h, c\n",
    "\n",
    "# rnn = ElmanRNN(input_size, hidden_size, nonlinearity)\n",
    "rnn = LSTM_RNN(input_size, hidden_size)\n",
    "\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4558f6-a563-470c-a7a6-e57417b39c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "n_epochs = 1000  # number of epochs\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    out, _, _  = rnn(coordinates[:-1])\n",
    "    error = loss(out, coordinates[1:])\n",
    "    error.backward()\n",
    "    optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Error at epoch {epoch} = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edadcd04-71dc-4344-8be3-930a435cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a trajectory\n",
    "n_points = 1000\n",
    "coordinate = torch.tensor([-1, 1], dtype=torch.float32, device=device)\n",
    "#coordinate = coordinates[0]\n",
    "coordinate = coordinate.unsqueeze(dim=0)\n",
    "\n",
    "def generate_trajectory(init_coordinate, n_points):\n",
    "    gen_coordinates = np.empty((n_points, 2))\n",
    "    coordinate = init_coordinate\n",
    "    h = torch.zeros((1, hidden_size), device=device)\n",
    "    c = torch.zeros((1, hidden_size), device=device)\n",
    "    with torch.no_grad():\n",
    "        for point_idx in range(n_points):\n",
    "            coordinate, h, c = rnn(coordinate, h, c)\n",
    "            gen_coordinates[point_idx, :] = coordinate.detach().cpu().numpy()\n",
    "    return gen_coordinates\n",
    "\n",
    "gen_coordinates = generate_trajectory(coordinate, n_points)\n",
    "all_generated.append(gen_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25ce85-17cb-4bfc-9ab4-79379e4b2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traces = len(all_generated)\n",
    "\n",
    "if n_traces > 3:\n",
    "    n_rows = int(np.floor(np.sqrt(n_traces + 1)))\n",
    "    n_cols = int(np.ceil((n_traces+1) / n_rows))\n",
    "else:\n",
    "    n_cols = n_traces + 1 \n",
    "    n_rows = 1\n",
    "fig, axs = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(5.1*n_cols,4.6*n_rows))\n",
    "for ax in axs:\n",
    "    if type(ax) == np.ndarray:\n",
    "        for _ax in ax:\n",
    "            _ax.set_aspect('equal')\n",
    "    else:\n",
    "        ax.set_aspect('equal')\n",
    "original = coordinates.cpu().detach()\n",
    "if n_rows > 1:\n",
    "    ax0 = axs[0,0]\n",
    "else:\n",
    "    ax0 = axs[0]\n",
    "ax0.plot(original[:,0], original[:,1])\n",
    "ax0.set_title('original trace')\n",
    "for trace in range(n_traces):\n",
    "    gen_coordinates = all_generated[trace]\n",
    "    if n_rows == 1:\n",
    "        ax = axs[trace+1]\n",
    "    else:\n",
    "        row = int(np.floor((trace+1)/n_cols))\n",
    "        col = int(np.ceil((trace+1)%n_cols)) \n",
    "        ax = axs[row, col]\n",
    "    ax.set_title(f'After {n_epochs*(trace+1)} epochs')\n",
    "    ax.plot(gen_coordinates[:,0], gen_coordinates[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167ecc9-f650-4bf9-b9d0-0f5061987c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a direction field\n",
    "lim_x = 2. # largest magnitude of x\n",
    "lim_y = 2.\n",
    "N_x = 2 # number of points in x dimension\n",
    "N_y = 2\n",
    "n_points = 800\n",
    "#df_fig = plt.figure(figsize = (10,10))\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.xlim(-1.1*lim_x, 1.1*lim_x)\n",
    "plt.ylim(-1.1*lim_y, 1.1*lim_y)\n",
    "plt.axis('equal')\n",
    "for x in np.linspace(-lim_x, lim_x, N_x):\n",
    "    for y in np.linspace(-lim_y, lim_y, N_y):\n",
    "        #print(f\"x = {x}, y = {y}\")\n",
    "        plt.scatter([x], [y], color='tab:pink')\n",
    "        coordinate = torch.tensor([x, y], dtype=torch.float32, device=device).unsqueeze(dim=0)\n",
    "        trajectory = generate_trajectory(coordinate, n_points)\n",
    "        plt.scatter([trajectory[0, 0]], [trajectory[0,1]], color='tab:cyan')\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fee9c39-ddc7-4eae-8c4e-52fe56c836da",
   "metadata": {},
   "source": [
    "# Dataset and Data Loader. Deprecated code.\n",
    "batch_size = 1  # This code is not adapted for batched input\n",
    "shuffle = False  # True would mess the hidden state\n",
    "\n",
    "class trace_dataset(Dataset):\n",
    "    \"\"\" Dataset for a sequence of 2D coordinates. \"\"\"\n",
    "    def __init__(self, trace_array:np.array):\n",
    "        # trace_array: Numpy array of shape (k, 2)\n",
    "        self.trace = torch.Tensor(trace_array) #.to(device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return a 2D coordinate (X1,Y1) and its next point (X2, Y2)\n",
    "        return self.trace[index], self.trace[index+1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.trace.shape[0] - 1\n",
    "\n",
    "dataset = trace_dataset(coordinates)\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, the optimizer, and the loss function\n",
    "input_size = 2  # this shouldn't change\n",
    "hidden_size = 256\n",
    "nonlinearity = 'tanh'  # tanh or relu\n",
    "lr = 3e-4  # learning rate\n",
    "all_generated = []\n",
    "\n",
    "torch.manual_seed(345)\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nonlinearity):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          nonlinearity=nonlinearity,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        if h0 is None:\n",
    "            h0 =  torch.zeros((1, hidden_size))\n",
    "        out, h = self.rnn(x, h0)\n",
    "        return self.fc(out), h\n",
    "\n",
    "rnn = ElmanRNN(input_size, hidden_size, nonlinearity)\n",
    "\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9c724-ad90-41a6-8895-effe02e6b0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
