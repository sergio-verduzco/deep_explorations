{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e72cd6-4d0d-4dd9-a049-b235a1026609",
   "metadata": {},
   "source": [
    "# latent.ipynb\n",
    "Maximizing the probability of training data by directly sampling $z$ from a normal distribution and estimating $P(X|z;\\theta)$ from that sample in order to approximate  \n",
    "$P(X) = \\int P(X|z;\\theta)P(z)dz \\quad$   using  \n",
    "$P(X|z_i; \\theta) \\approx \\sum_j P(z_i) \\frac{\\mathbf{1}(X_{ij}, X)}{N}$,  \n",
    "where $\\mathbf{1}(X_{ij}, X) = 1$ if $X_{ij} = X$ and $\\mathbf{1}(X_{ij}, X) = 0$ otherwise.\n",
    "\n",
    "There are at least 3 ways to proceed:  \n",
    "1) If $f(z, \\theta)$ is deterministic, we take a grid of M values of $z$ in the interval $[z_{min}, z_{max}]$. For each value $X_i$ in the training data we find the values $z_i$ s.t. $f(z_i, \\theta) = X_i$. We then approximate $P(X_i) = \\Delta z \\sum_i P(z_i)$. This is the same as taking each one of the M $z_i$, values, obtaining $X_i$, and updating $P(X_i) \\leftarrow P(X_i) + \\Delta z P(z_i)$. \n",
    "2) We initialize $P(X_j) = 0$ for all $X_j$ in the training set. We take a grid of M values of $z$ in the interval $[z_{min}, z_{max}]$. For each value $z_i$ in the grid we sample $X_i = f(z_i, \\theta)$. Each time $X_i$ is equal to a value $X_j$ in the training set we increase $P(X_j)$ by an amount $P(X_j|z_i, \\theta)P(z_i)$. At the end, we normalize all the $P(X_j)$ values so they add to 1.\n",
    "3) Sample the $z_i$ stochastically from the normal distribution. Each time an $X_i$ is computed, and $P(X_i)$ is increased by an amount $\\epsilon$. After sampling you normalize the $P(X_i)$ values so they add to 1. This may not be differentiable...\n",
    "\n",
    "The training data consists of 10-element vectors of 1's and 0's, where only two 1's are present at random locations. The latent space has dimension 2.  \n",
    "We use a 3-layer feedforward perceptron to map from the $z$ to the $X$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3813c5-1e28-4c62-bac4-2ccba7fb6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7ce76-907c-42a0-881d-73fcd874f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFP(nn.Module):\n",
    "    \"\"\" A feedforward perceptron. \"\"\"\n",
    "    def __init__(self, sizes, nltypes, bias=True):\n",
    "        \"\"\"\n",
    "            sizes: list with size of each layer.\n",
    "            nltypes: list with nonlinearity type for each inner or\n",
    "                output layer. Entries are 'relu', 'sig', or 'tanh'.\n",
    "            bias: whether the layers have a bias unit\n",
    "        \"\"\"\n",
    "        assert len(sizes)-1 == len(nltypes), \"length mismatch in nltypes, sizes\"\n",
    "        super(FFP, self).__init__()\n",
    "        # Add activation functions\n",
    "        self.nlfs = []\n",
    "        for nltype in nltypes:\n",
    "            if nltype == \"relu\":\n",
    "                self.nlfs.append(nn.ReLU())\n",
    "            elif nltype == \"sig\":\n",
    "                self.nlfs.append(nn.Sigmoid())\n",
    "            elif nltype == \"tanh\":\n",
    "                self.nlfs.append(nn.Tanh())\n",
    "            else:\n",
    "                raise ValueError(f\"unknown nonlinearity {nltype}\")\n",
    "        # create layers\n",
    "        self.bias = bias\n",
    "        self.sizes = sizes\n",
    "        layers = []\n",
    "        for lidx in range(1,len(sizes)):\n",
    "            layers.append(nn.Linear(sizes[lidx-1], sizes[lidx], bias=bias))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        for lidx, layer in enumerate(self.layers):\n",
    "            x = self.nlfs[lidx](layer(x))\n",
    "        return x\n",
    "\n",
    "class standard_SGD():\n",
    "    \"\"\" An SGD optimizer for my FFP module. \"\"\"\n",
    "    def __init__(self, model, lr=0.1):\n",
    "        \"\"\"\n",
    "            model: an instance of the FFP class\n",
    "            lr: learning rate\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\" Updates the model's parameters. \"\"\"\n",
    "        for lidx, layer in enumerate(self.model.layers, 1):\n",
    "            dw = self.lr * layer.weight.grad\n",
    "            with torch.no_grad():\n",
    "                layer.weight -= dw\n",
    "                if self.model.bias:\n",
    "                    layer.bias -= self.lr * layer.bias.grad\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for layer in self.model.layers:\n",
    "            layer.weight.grad.zero_()\n",
    "            if self.model.bias:\n",
    "                layer.bias.grad.zero_()\n",
    "\n",
    "def loss("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c94de78-000a-42bc-9bf1-d6ff240943d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Define the sample interval for z\n",
    "# 2) for all z values\n",
    "    # 2.1) Choose a z value\n",
    "    # 2.2) Obtain the corresponding X value\n",
    "    # 2.3) Update P(X) with P(z)\n",
    "# 3) Test whether all X values have P(X) > 0\n",
    "# 4) Change the parameters using the gradients for all P(X) functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3ba7e-46db-4acc-a4de-53b6145a7dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
