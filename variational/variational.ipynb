{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e72cd6-4d0d-4dd9-a049-b235a1026609",
   "metadata": {},
   "source": [
    "# variational.ipynb\n",
    "A variational autoenconder writing the loss function from Eq. (7) in   \n",
    "[Doersch 2016, Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "\n",
    "\n",
    "-Sergio Verduzco  \n",
    "June 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8250b-a8e9-46be-9983-eafca26ae91c",
   "metadata": {},
   "source": [
    "### Some resources I consulted:\n",
    "https://youtu.be/uaaqyVS9-rM  \n",
    "https://www.youtube.com/watch?v=YV9D3TWY5Zo  \n",
    "https://www.youtube.com/watch?v=8wrLjnQ7EWQ  \n",
    "https://www.youtube.com/watch?v=VELQT1-hILo  \n",
    "https://github.com/karpathy/examples/blob/master/vae/main.py  \n",
    "https://arxiv.org/pdf/1906.02691.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3813c5-1e28-4c62-bac4-2ccba7fb6951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6cf96-4925-4352-a104-6b9e57498600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preliminary parameters\n",
    "data_dir = '/home/z/Downloads/data/'  \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}\")\n",
    "batch_size = 64\n",
    "dataset = 'MNIST'  # MNIST. CIFAR10 not implemented yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f6263-78bf-48fc-9d9a-4be4e3787d5d",
   "metadata": {},
   "source": [
    "Tricky things with the cell above:\n",
    "* Batch size\n",
    "\n",
    "\n",
    "Tricky things with the cell below:\n",
    "* Which normalization value to use?\n",
    "  * Must ensure it is consistent with the output nonlinearity of the decoder\n",
    "  * Turns out `ToTensor` sets the values between 0 and 1, and this is enough. Further normalizing may hurt performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30407f-1787-4a5e-aea2-6f97e4c84613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset == 'CIFAR10':  # ignore!\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "    #testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "elif dataset == 'MNIST':\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         #transforms.Normalize((1.), (0.5)),\n",
    "         torch.squeeze])\n",
    "    trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    #testset = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "else:\n",
    "    raise ValueError(\"Specify a valid dataset\")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "# testloader = torch.utils.data.DataLoader(testset,\n",
    "#                                          batch_size=batch_size,\n",
    "#                                          shuffle=False,\n",
    "#                                          num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e61c8-3c35-4600-9ee2-8c3fb6526572",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.randn_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7ce76-907c-42a0-881d-73fcd874f1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFP(nn.Module):\n",
    "    \"\"\" A feedforward perceptron. \"\"\"\n",
    "    def __init__(self, sizes, nltypes, bias=True):\n",
    "        \"\"\"\n",
    "            sizes: list with size of each layer.\n",
    "            nltypes: list with nonlinearity type for each inner or\n",
    "                output layer. Entries are 'relu', 'sig', or 'tanh'.\n",
    "            bias: whether the layers have a bias unit\n",
    "        \"\"\"\n",
    "        assert len(sizes)-1 == len(nltypes), \"length mismatch in nltypes, sizes\"\n",
    "        super(FFP, self).__init__()\n",
    "        # Add activation functions\n",
    "        self.nlfs = []\n",
    "        for nltype in nltypes:\n",
    "            if nltype == \"relu\":\n",
    "                self.nlfs.append(nn.ReLU())\n",
    "            elif nltype == \"sig\":\n",
    "                self.nlfs.append(nn.Sigmoid())\n",
    "            elif nltype == \"tanh\":\n",
    "                self.nlfs.append(nn.Tanh())\n",
    "            elif nltype == \"linear\":\n",
    "                self.nlfs.append(\"linear\")\n",
    "            else:\n",
    "                raise ValueError(f\"unknown nonlinearity {nltype}\")\n",
    "        # create layers\n",
    "        self.bias = bias\n",
    "        self.sizes = sizes\n",
    "        layers = []\n",
    "        for lidx in range(1,len(sizes)):\n",
    "            layers.append(nn.Linear(sizes[lidx-1], sizes[lidx], bias=bias))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        for lidx, layer in enumerate(self.layers):\n",
    "            if self.nlfs[lidx] == \"linear\":\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = self.nlfs[lidx](layer(x))\n",
    "        return x\n",
    "\n",
    "class normal_encoder(nn.Module):\n",
    "    \"\"\" A FFP that outputs parameters for a multivariate normal distribution.\n",
    "    \n",
    "        This is the same as the FFP class, except that the output layer is\n",
    "        duplicated by concatenating an extra set of units with sigmoidal \n",
    "        activation functions. This extra set of units corresponds to the\n",
    "        entries of a diagonal covariance matrix, whereas the regular outputs\n",
    "        corresponds to the means.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes, nltypes, bias=True):\n",
    "        \"\"\"\n",
    "            sizes: list with size of each layer.\n",
    "            nltypes: list with nonlinearity type for each inner or\n",
    "                output layer. Entries are 'relu', 'sig', 'tanh', or 'linear'.\n",
    "            bias: whether the layers have a bias unit\n",
    "        \"\"\"\n",
    "        assert len(sizes)-1 == len(nltypes), \"length mismatch in nltypes, sizes\"\n",
    "        super(normal_encoder, self).__init__()\n",
    "        self.n_layers = len(nltypes)\n",
    "        # Add activation functions\n",
    "        self.nlfs = []\n",
    "        for nltype in nltypes:\n",
    "            if nltype == \"relu\":\n",
    "                self.nlfs.append(nn.ReLU())\n",
    "            elif nltype == \"sig\":\n",
    "                self.nlfs.append(nn.Sigmoid())\n",
    "            elif nltype == \"tanh\":\n",
    "                self.nlfs.append(nn.Tanh())\n",
    "            elif nltype == \"linear\":\n",
    "                self.nlfs.append(\"linear\")\n",
    "            else:\n",
    "                raise ValueError(f\"unknown nonlinearity {nltype}\")\n",
    "        # create layers\n",
    "        self.bias = bias\n",
    "        self.sizes = sizes\n",
    "        layers = []\n",
    "        for lidx in range(1,len(sizes)):\n",
    "            layers.append(nn.Linear(sizes[lidx-1], sizes[lidx], bias=bias))\n",
    "        # the last element in layers will be the sigmoidal variance layer\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=bias))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        for lidx, layer in enumerate(self.layers[:-1]):\n",
    "            if lidx == self.n_layers - 1:\n",
    "                y = nn.Sigmoid()(self.layers[-1](x))\n",
    "            if self.nlfs[lidx] == \"linear\":\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = self.nlfs[lidx](layer(x))\n",
    "        return torch.concatenate((x, y), axis=-1)\n",
    "        \n",
    "class standard_SGD():\n",
    "    \"\"\" An SGD optimizer for my FFP module. \"\"\"\n",
    "    def __init__(self, model, lr=0.1):\n",
    "        \"\"\"\n",
    "            model: an instance of the FFP class\n",
    "            lr: learning rate\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\" Updates the model's parameters. \"\"\"\n",
    "        for lidx, layer in enumerate(self.model.layers, 1):\n",
    "            dw = self.lr * layer.weight.grad\n",
    "            with torch.no_grad():\n",
    "                layer.weight -= dw\n",
    "                if self.model.bias:\n",
    "                    layer.bias -= self.lr * layer.bias.grad\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for layer in self.model.layers:\n",
    "            layer.weight.grad.zero_()\n",
    "            if self.model.bias:\n",
    "                layer.bias.grad.zero_()\n",
    "\n",
    "def KL_Loss(mu:torch.tensor, sigma:torch.tensor, reduction='mean') -> float:\n",
    "    \"\"\" Encoder loss from the KL divergence.\n",
    "\n",
    "        The loss is the KL divergence between two multivariate\n",
    "        normals. The first has the given means, and a diagonal \n",
    "        variance matrix with the given sigma values. The second\n",
    "        is a multivariate normal zero mean and identity covariance\n",
    "        matrix.\n",
    "\n",
    "        One term that is not relevant for the computation of gradients\n",
    "        is removed from the loss (-k).\n",
    "        \n",
    "        Args:\n",
    "            mu: mean values, size (m,k), where m=minibatch size, k=dimension\n",
    "            sigma: variances, size (m,k)\n",
    "                   All values must be positive.\n",
    "            reduction: 'mean', 'sum', or None\n",
    "        Returns:\n",
    "            loss: KL divergence\n",
    "    \"\"\"\n",
    "    loss =  0.5 * (sigma.sum(axis=1) + (mu * mu).sum(axis=1) - sigma.prod(axis=1).log())\n",
    "    \n",
    "    if loss.dim() == 0:\n",
    "        return loss\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    if reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    if reduction == None:\n",
    "        return loss\n",
    "    raise ValueError(\"type of reduction was not understood.\")\n",
    "\n",
    "def distro_loss(mu:torch.tensor, sigma:torch.tensor, reduction='mean') -> float:\n",
    "    \"\"\" A MSE loss on the parameters of the multivariate normal.\n",
    "    \n",
    "    Args:\n",
    "            mu: mean values, size (m,k), where m=minibatch size, k=dimension\n",
    "            sigma: variances, size (m,k)\n",
    "                   All values must be positive.\n",
    "            reduction: 'mean', 'sum', or None\n",
    "        Returns:\n",
    "            norm of mu plus norm of (sigma - 1)\n",
    "    \"\"\"\n",
    "    loss = (mu * mu).sum() + (sigma - 1).pow(2)\n",
    "\n",
    "    if loss.dim() == 0:\n",
    "        return loss\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    if reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    if reduction == None:\n",
    "        return loss\n",
    "    raise ValueError(\"type of reduction was not understood.\")\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593c9df-4d21-403b-a94c-4406f40278ad",
   "metadata": {},
   "source": [
    "Things that caused trouble with the cell above:\n",
    "* Handling the separation in the last encoder layer. Half the outputs are mu, half are sigma.\n",
    "* Ensuring that the sigma values of the encoder are always positive, and finding the right nonlinearity to use for the mu values.\n",
    "  * The answer is to use sigmoidal activation for sigma, and a linear layer for mu.\n",
    "\n",
    "\n",
    "Things that caused trouble with the cell below:\n",
    "* Size of the network and type of nonlinearity for each layer.\n",
    "  * Last layer of encoder must be able to produce both positive and negative values for mu, but only positive for sigma.\n",
    "  * One video said the last layer of the decoder should use sigmoid units because of the MNIST image encoding. When I was normalizing the input images, nothing worked for me until I used a linear output layer. After I stoped normalizing a sigmoidal was probably better.\n",
    "* Optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0eafbd-8cec-41a0-a113-ac5b426f52fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the encoder and decoder\n",
    "n_latent = 2  # number of latent variables\n",
    "enc_sizes = [784, 200, 100, n_latent]\n",
    "enc_types = ['tanh', 'relu', 'linear']\n",
    "encoder = normal_encoder(enc_sizes, enc_types, bias=True).to(device)\n",
    "\n",
    "dec_sizes = [n_latent, 100, 200, 784]\n",
    "dec_types = ['tanh', 'relu', 'linear']\n",
    "decoder = FFP(dec_sizes, dec_types, bias=True).to(device)\n",
    "\n",
    "# Multivariate normal\n",
    "assert enc_sizes[-1] == dec_sizes[0], \"Check bottleneck sizes\"\n",
    "\n",
    "std_multi_normal = torch.distributions.MultivariateNormal(torch.zeros(n_latent), torch.eye(n_latent))\n",
    "\n",
    "# Optimizers\n",
    "# encoder_optim = standard_SGD(encoder, lr=0.001)\n",
    "# decoder_optim = standard_SGD(decoder, lr=0.001)\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), lr=5e-4)\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters(), lr=5e-4)\n",
    "# encoder_optim = torch.optim.SGD(encoder.parameters(), lr=0.1)\n",
    "# decoder_optim = torch.optim.SGD(decoder.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1017f23-ce56-4752-bc85-f95d37eb4300",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "n_epochs = 10\n",
    "bsize = trainloader.batch_size\n",
    "w_bit = 1e-5\n",
    "w = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    accum_distrib_error = 0.\n",
    "    accum_output_error = 0.\n",
    "    print(\"----------------------------------------\")\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # retrieve the input\n",
    "        input = data[0].flatten(start_dim=1).to(device)  # (bsize, 784)\n",
    "        # feed the input to the encoder\n",
    "        latent = encoder(input)  # (bsize, 2*n_latent)\n",
    "        # extract means and diagonal of variance matrix\n",
    "        means = latent[:, :enc_sizes[-1]]  # (bsize, n_latent)\n",
    "        vars = latent[:, enc_sizes[-1]:]  # (bsize, n_latent)\n",
    "        # sample from the multivariate normal. Size = (bsize, n_latent)\n",
    "        #epsilons = std_multi_normal.sample((bsize,)).to(device)\n",
    "        epsilons = torch.randn_like(vars)\n",
    "        z = means + epsilons * vars\n",
    "        # feed the sample to the decoder\n",
    "        output = decoder(z)\n",
    "        # Gradient descent\n",
    "        distrib_error = w * KL_Loss(means, vars)\n",
    "        # distrib_error = w * distro_loss(means, vars)\n",
    "        output_error = mse_loss(input, output)\n",
    "        #output_error = F.binary_cross_entropy(input, output, reduction='mean')\n",
    "        error = distrib_error + output_error\n",
    "        \n",
    "        error.backward()\n",
    "        \n",
    "        encoder_optim.step()\n",
    "        decoder_optim.step()\n",
    "\n",
    "        encoder_optim.zero_grad()\n",
    "        decoder_optim.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w += w_bit * (output_error - w * distrib_error)\n",
    "\n",
    "        # display error\n",
    "        accum_distrib_error += distrib_error\n",
    "        accum_output_error += output_error\n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f\"    distribution error = {accum_distrib_error/i} up to example {i*bsize}\")\n",
    "            print(f\"    reconstruction error = {accum_output_error/i} up to example {i*bsize}\")\n",
    "    accum_distrib_error /= len(trainloader)\n",
    "    accum_output_error /= len(trainloader)\n",
    "    print(f\"distribution error = {accum_distrib_error} in epoch {epoch}\")\n",
    "    print(f\"reconstruction error = {accum_output_error} in epoch {epoch}\")\n",
    "    print(f\"w = {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7102a6d-cbca-45a2-b20d-2f695378de8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class random_image_generator():\n",
    "    def __init__(self, decoder, distribution):\n",
    "        self.decoder = decoder\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def generate(self, show=True):\n",
    "        input = self.distribution.sample().to(device)\n",
    "        \n",
    "        output = decoder(input).reshape((28,28)).to('cpu').detach()\n",
    "        if show:\n",
    "            plt.imshow(output)\n",
    "        return output\n",
    "\n",
    "# empirical_multi_normal = torch.distributions.MultivariateNormal(0.0*torch.ones(n_latent), 0.7*torch.eye(n_latent))\n",
    "# imgen = random_image_generator(decoder, empirical_multi_normal)\n",
    "\n",
    "imgen = random_image_generator(decoder, std_multi_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e85f64-a431-49ae-a31d-3f2a4b89632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(5)\n",
    "b = torch.tensor([6, 7, 8, 9, 0])\n",
    "a = torch.tensor(a)\n",
    "print(a.p)\n",
    "print(b.p)\n",
    "\n",
    "a.pow(2).div(b +.001).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3ba7e-46db-4acc-a4de-53b6145a7dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run repeatedly to get many images\n",
    "val = imgen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f21ddd-bd50-4f41-8d38-dfab259ad47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of images (when n_latent=2)\n",
    "min_z1 = -2.\n",
    "max_z1 = 2.\n",
    "min_z2 = -2.\n",
    "max_z2 = 2.\n",
    "n_grid = 10\n",
    "fig, axs = plt.subplots(ncols=n_grid, nrows=n_grid, figsize=(n_grid, n_grid))\n",
    "for idx1, z1 in enumerate(np.linspace(min_z1, max_z1, n_grid)):\n",
    "    for idx2, z2 in enumerate(np.linspace(min_z2, max_z2, n_grid)):\n",
    "        input = torch.tensor([z1, z2], dtype=torch.float32).to(device)\n",
    "        output = decoder(input).reshape((28,28)).to('cpu').detach()\n",
    "        axs[idx1, idx2].set_xticks([])\n",
    "        axs[idx1, idx2].set_yticks([])\n",
    "        axs[idx1, idx2].imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032515dd-c10e-494b-aa5b-1fb4951cba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of images (when n_latent=1)\n",
    "min_z1 = -2.\n",
    "max_z1 = 2.\n",
    "n_grid = 15\n",
    "fig, axs = plt.subplots(ncols=n_grid, nrows=1, figsize=(n_grid, n_grid))\n",
    "for idx1, z1 in enumerate(np.linspace(min_z1, max_z1, n_grid)):\n",
    "    input = torch.tensor([z1], dtype=torch.float32).to(device)\n",
    "    output = decoder(input).reshape((28,28)).to('cpu').detach()\n",
    "    axs[idx1].set_xticks([])\n",
    "    axs[idx1].set_yticks([])\n",
    "    axs[idx1].imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a823af-0ce1-440f-b867-d161b02020de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the decoder's statistitics. Should look like the \"input\" next cell\n",
    "print(val)\n",
    "#print(std_multi_normal.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a836279-58bb-4487-ae93-de6d8c8bc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the encoder's output statistics\n",
    "sample_num = 5\n",
    "\n",
    "for n in range(sample_num):\n",
    "    input = trainset[n][0].flatten().reshape(1,-1).to(device)\n",
    "    latent = encoder(input)\n",
    "    mu = latent[0, :n_latent]\n",
    "    sigma = latent[0, n_latent:]\n",
    "    print(\"input = \", end=\" \")\n",
    "    print(input)\n",
    "    print(\"mu = \", end=\" \")\n",
    "    print(mu)\n",
    "    print(\"sigma = \", end=\" \")\n",
    "    print(sigma)\n",
    "    print(\"==========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fc33a-8d25-49f1-822b-d7cc67cc8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), 'decoder_distro_loss01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e223151-02a4-427c-872b-1578f1a1e4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
